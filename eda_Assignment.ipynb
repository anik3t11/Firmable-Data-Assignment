{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Required Library"
      ],
      "metadata": {
        "id": "OmugdXgvVefD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lkrVJ1qVVEx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import bigquery\n",
        "from datetime import date\n",
        "drive.mount('/content/drive') #mounting g drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Library"
      ],
      "metadata": {
        "id": "JLieekeSVsK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "8p7KzBiJVZIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter for visualization\n",
        "The following settings will improve the default style and font sizes for our charts."
      ],
      "metadata": {
        "id": "mc3-0A60Vxjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('darkgrid') #theme of graph\n",
        "matplotlib.rcParams['font.size'] = 14 #fontsize displayed in graph\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 6) #Chart size parameter\n",
        "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
      ],
      "metadata": {
        "id": "RF2fhuGmVZSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up conection between notebook and Database\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/pran-project-470608-2f7bfba6b03c.json'\n",
        "client = bigquery.Client.from_service_account_json(folder_path)"
      ],
      "metadata": {
        "id": "fmrrhQkDVZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data from Database using the connection we setup"
      ],
      "metadata": {
        "id": "Jx3kz7tZYKrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"\"\"\n",
        "select * from firmable.data_table\n",
        "\"\"\"\n",
        "df = client.query(query1).to_dataframe()\n",
        "print(\"size of dataframe\",df.shape)"
      ],
      "metadata": {
        "id": "ZK0sW4KuVZm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = \"select * from firmable.company_details\"\n",
        "df2 = client.query(query2).to_dataframe()\n",
        "print(\"size of dataframe\",df2.shape)"
      ],
      "metadata": {
        "id": "nwQGrDtxVZwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = \"select * from firmable.article_details\"\n",
        "df3 = client.query(query3).to_dataframe()\n",
        "print(\"size of dataframe\",df3.shape)"
      ],
      "metadata": {
        "id": "QNfITIRFVZ4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df1 620785 and df3 has 577277  rows while df2 has 148630 rows as duplicate rows were in ingestion code"
      ],
      "metadata": {
        "id": "68Kx8yAPiDuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging all dataframe so that we can work on single dataframe"
      ],
      "metadata": {
        "id": "aD7pC5VoYU6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_df = df.merge(df2,left_on = 'relationship_company1_id',right_on = 'id',how = 'left',suffixes = ('','_company_1'))\n",
        "\n",
        "final_merge = merge_df.merge(df2,left_on = 'relationship_company2_id',right_on = 'id',how = 'left',suffixes = ('','_company_2'))\n",
        "\n",
        "final_df = final_merge.merge(df3,left_on ='relationship_most_relevant_id',right_on='id',how= 'left',suffixes = ('','_article') )\n",
        "print(\"size of dataframe\",final_df.shape)"
      ],
      "metadata": {
        "id": "B4qUvlmDVaAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "after merging of all three rows we are getting 620785 rows\n",
        "As every article id should be unique to every event as article_table< data_table which indicate presnt of duplicate article_id in dataset"
      ],
      "metadata": {
        "id": "0ec-u3ACidYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head(5)"
      ],
      "metadata": {
        "id": "1tfZHyLEZPPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Missing value"
      ],
      "metadata": {
        "id": "gpCUrVLHZW9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_column_count = pd.DataFrame({\n",
        "    'null_value': final_df.isnull().sum(),\n",
        "    'total_rows':len(final_df),\n",
        "    'null_percentage': (final_df.isnull().sum() / len(final_df)* 100).round(2)\n",
        "})\n",
        "\n",
        "null_column_count= null_column_count.sort_values(by = ['null_percentage','null_value'],ascending = [False,False])\n",
        "null_column_count"
      ],
      "metadata": {
        "id": "QwlN9bVVZRWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyisng the dataframe above\n",
        "we can say column division has all the value null\n",
        "there are many other column as well like product_version,financing_type_normalized,headcount,assets,award,event,product_release_type with more than 95% values are null\n",
        "\n",
        "We can't drop this column or fill the value in this as all the rows are mutually exlusive to each other"
      ],
      "metadata": {
        "id": "-0ZpKcpiizGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.dtypes"
      ],
      "metadata": {
        "id": "iRiEo1B9ZRds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataTypes Check\n",
        "Effective date column is in dbDate it only represnt date part only\n",
        "and we can't directly compare column with datetime column\n",
        "\n",
        "product_fuzzy_match value has few null column else it contain Boolean valuer that alos need to be corrected"
      ],
      "metadata": {
        "id": "UVXL6U1-kxoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# temp_df = final_df.copy()\n",
        "# temp_df[\"effective_date\"] = pd.to_datetime(temp_df[\"effective_date\"], errors=\"coerce\", utc=True)\n",
        "# temp_df.dtypes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k8hspRtsj5Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The describe() function in pandas provides the statistical distribution of all numeric columns in the dataset.\n",
        "\n",
        "From the output, we observe the following\n",
        "\n",
        "1. Headcount is currently stored as a float datatype. Since headcount represents the number of individuals, it should be stored as an integer. Additionally, the minimum value of headcount is negative, which is not logically possible and should be treated as a data quality issue.\n",
        "\n",
        "\n",
        "2. Amount_normalized also contains negative values, which are invalid in the given business context and should be corrected or removed during data cleaning.\n",
        "\n",
        "3. Confidence_score is stored as a float and is correctly constrained within the range [0, 1], which aligns with its definition as a probability or model score."
      ],
      "metadata": {
        "id": "_0ywNK8OljFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.describe()"
      ],
      "metadata": {
        "id": "9cLplRxqZRkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution of categry\n",
        "\n",
        "Here we will be checking Distribution of major column values"
      ],
      "metadata": {
        "id": "xIo5S9lKac9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.category.value_counts(ascending = False)"
      ],
      "metadata": {
        "id": "rFPuBy83ZRp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(final_df['category'].unique(), final_df['category'].value_counts())\n",
        "\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.xlabel(\"Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(final_df['category'].unique(),rotation = 90)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V7jpIOM8bUqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of news over the year"
      ],
      "metadata": {
        "id": "ZuS-a26jaglI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " year_count = final_df.copy()\n",
        "# Ensure found_date is datetime\n",
        "year_count[\"found_date\"] = pd.to_datetime(year_count[\"found_date\"], errors=\"coerce\")\n",
        "\n",
        "# Extract year-month\n",
        "year_count[\"year\"] = year_count[\"found_date\"].dt.year\n",
        "\n",
        "# Count how many news ingested in each year-month\n",
        "news_counts = year_count.groupby(\"year\").size().reset_index(name = \"news_count\").sort_values(\"year\")\n",
        "\n",
        "news_counts"
      ],
      "metadata": {
        "id": "Ec8R5fYZZRvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(news_counts['year'], news_counts['news_count'])\n",
        "\n",
        "plt.title(\"Number of News Ingested per Year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"News Count\")\n",
        "plt.xticks(news_counts['year'], rotation=45)  # show year on X-axis\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add labels to the bars\n",
        "for index, row in news_counts.iterrows():\n",
        "    plt.text(row['year'], row['news_count'], str(row['news_count']), ha='center', va='bottom', rotation=90)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3qi_0WmOZRz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count of Article id to check repetation as it should be unique"
      ],
      "metadata": {
        "id": "Nq0d0GgIsf9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.id_article.value_counts(ascending = False)"
      ],
      "metadata": {
        "id": "kaYlbQ0SZR5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When we checked distribution of article id we found out ther are duplicate article id present\n",
        "which Directly hints towards duplicacy of rows in dataframe**"
      ],
      "metadata": {
        "id": "R9Tz2CxkpWH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy\n",
        "\n",
        "Check wether article published correctly represnt actual news and facts"
      ],
      "metadata": {
        "id": "T4TrtOaZssLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with date as news which is ingested into system should happen or published the ingestion date"
      ],
      "metadata": {
        "id": "xA31RRScs_8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_accuracy_check = final_df.copy()\n",
        "date_accuracy_check['found_date'] = pd.to_datetime(date_accuracy_check['found_date'],utc= True)\n",
        "date_accuracy_check['effective_date']= pd.to_datetime(date_accuracy_check['effective_date'],utc= True)\n",
        "date_accuracy_check['published_at']= pd.to_datetime(date_accuracy_check['published_at'],utc= True)\n",
        "\n",
        "date_accuracy_check['reference_date'] = date_accuracy_check['effective_date'].fillna(\n",
        "    date_accuracy_check['published_at']\n",
        ")\n",
        "\n",
        "date_inconsitency_count = date_accuracy_check[\n",
        "    (date_accuracy_check['reference_date'].dt.date > date_accuracy_check['found_date'].dt.date) &\n",
        "    (date_accuracy_check['planning'] == False)\n",
        "].shape[0]\n",
        "\n",
        "total_count = len(date_accuracy_check)\n",
        "\n",
        "date_accuracy = (1 - (date_inconsitency_count / total_count)) * 100\n",
        "\n",
        "print(f\"Date Accuracy: {date_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "2o6IPz6JZSCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering confidence score and Human approved as confidence score is been derived from ML model may be wrong is some case and high confidence score must match with Human approval"
      ],
      "metadata": {
        "id": "3MvNDOtquwTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_conf = date_accuracy_check[\"confidence_score\"]>0.8\n",
        "\n",
        "# if high confidence is true and human approved  is true or high confidence is false or human approved is false that will give us true\n",
        "agreement = ((high_conf & date_accuracy_check[\"human_approved\"]) | (~high_conf & ~date_accuracy_check[\"human_approved\"]))\n",
        "aggreement_accuracy = agreement.mean()*100\n",
        "\n",
        "print(f\"Confidence and Human approved: {aggreement_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "p-QUN68ouojy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Accuracy percentage"
      ],
      "metadata": {
        "id": "qT8-leROv7Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (date_accuracy + aggreement_accuracy)/2\n",
        "print(f\"Overall Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "CYdfbnviz5eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completeness\n",
        "\n",
        "Missing or Null value Present in dataframe"
      ],
      "metadata": {
        "id": "QGDks-wbIH6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#give overall summary of null and not null value presnt in our data frame\n",
        "null_column_count = pd.DataFrame({\n",
        "    'null_value': final_df.isnull().sum(),\n",
        "    'total_rows':len(final_df),\n",
        "    'null_percentage': (final_df.isnull().sum() / len(final_df)* 100).round(2)\n",
        "})\n",
        "\n",
        "null_column_count= null_column_count.sort_values(by = ['null_percentage','null_value'],ascending = [False,False])\n",
        "\n",
        "null_column_count"
      ],
      "metadata": {
        "id": "DBHPdyiM2Bmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Give us overall null and not null percentage\n",
        "\n",
        "# Data is discrete so null value can't be filled with help of other columns\n",
        "\n",
        "overall_null_percentage = (null_column_count['null_value'].sum()/\n",
        "                           null_column_count['total_rows'].sum())*100\n",
        "\n",
        "print(f\"Overall Null Percentage: {overall_null_percentage:.2f}%\")\n",
        "\n",
        "overall_complete = 100 -overall_null_percentage\n",
        "\n",
        "print(f\"Overall Complete: {overall_complete:.2f}%\")\n"
      ],
      "metadata": {
        "id": "9vDv-7uEQtFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consistency\n",
        "\n",
        "Wether value follows same format  across the dataset"
      ],
      "metadata": {
        "id": "dogUaw3tIPQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking category column format across\n",
        "final_df.category.value_counts(ascending = False)"
      ],
      "metadata": {
        "id": "cO7C-15-Sy39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Category column is in consistent format"
      ],
      "metadata": {
        "id": "ry_6UbKvDuSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.planning.value_counts(ascending = False)"
      ],
      "metadata": {
        "id": "E26UZ-MNVT2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if date is consistent"
      ],
      "metadata": {
        "id": "R0DK-8WjD6Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_cols = [\"found_date\",\"effective_date\",\"published_at\"]\n",
        "\n",
        "date_consistency = {}\n",
        "\n",
        "for col in date_cols:\n",
        "    if col in final_df.columns:\n",
        "      parsed = pd.to_datetime(final_df[col],errors = \"coerce\",utc=True)\n",
        "      valid_ratio = parsed.notna().mean()*100\n",
        "      date_consistency[col] = valid_ratio\n",
        "\n",
        "print(\"Date Consistency\",date_consistency)"
      ],
      "metadata": {
        "id": "rSHR4xsbVsEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timeliness\n",
        "\n",
        "Data is been updated timely in Database"
      ],
      "metadata": {
        "id": "RVLIgQU5Vyw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_date_data = final_df[date_cols]\n",
        "\n",
        "check_date_data.head(20)"
      ],
      "metadata": {
        "id": "tzDjlHJ3Hv3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering the data so we will have correct analysis basically remoing the rows where effective date is null\n",
        "# Selecting the rows where ingestion date > effective date to have better analysis\n",
        "date_check_data = (\n",
        "    (final_df[\"planning\"] == False) & (final_df[\"effective_date\"].notna()) &\n",
        "    (\n",
        "        pd.to_datetime(final_df[\"found_date\"], utc=True) >= pd.to_datetime(final_df[\"effective_date\"], utc=True)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Convert to datetime only for the calculation (does not overwrite columns)\n",
        "found_dt = pd.to_datetime(final_df.loc[date_check_data, \"found_date\"], errors=\"coerce\", utc=True)\n",
        "eff_dt   = pd.to_datetime(final_df.loc[date_check_data, \"effective_date\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "# Calculate lag in days\n",
        "lag_days = (found_dt - eff_dt).dt.days\n",
        "\n",
        "# Mean lag\n",
        "mean_lag = lag_days.mean()\n",
        "max_lag = lag_days.max()\n",
        "min_lag = lag_days.min()\n",
        "median_lag =lag_days.median()\n",
        "\n",
        "\n",
        "print(f\"Average lag (found_date - effective_date): {mean_lag:.2f} days\")\n",
        "print(f\"max lag (found_date - effective_date): {max_lag:.2f} days\")\n",
        "print(f\"min lag (found_date - effective_date): {min_lag:.2f} days\")\n",
        "print(f\"median lag (found_date - effective_date): {median_lag:.2f} days\")"
      ],
      "metadata": {
        "id": "ruVwlbHLI-a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the date in box plot to identify the distribution of time across\n",
        "# Used Plotly Library so that we can have better labels\n",
        "fig = px.box(lag_days,\n",
        "             y=lag_days,\n",
        "             points=\"outliers\",\n",
        "             title=\"Lag between Found Date and Effective Date & Planning = False\")\n",
        "fig.update_yaxes(title=\"Lag (days)\")"
      ],
      "metadata": {
        "id": "syxKLC-7N3Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uniqueness\n",
        "\n",
        "Duplicacy of Data presnt in dataframe\n",
        "\n",
        "By analysis above we have figured out presnce of duplicate data in few of the column we will,be doing analysis on that selected column only"
      ],
      "metadata": {
        "id": "RgdzChiwXACm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_unique = final_df[\"id\"].nunique()/len(final_df)*100\n",
        "print(f\"Uniquesness of id: {id_unique:.2f}%\")\n",
        "\n",
        "id_article_uniqueness = final_df[\"id_article\"].nunique()/len(final_df)*100\n",
        "print(f\"article_id uniqueness: {id_article_uniqueness:.2f}%\")\n",
        "\n",
        "\n",
        "combo_uniquness = final_df[[\"summary\",\"found_date\"]].drop_duplicates().shape[0]/len(final_df)*100\n",
        "print(f\"combo_uniquness : {combo_uniquness:.2f}%\")\n",
        "\n",
        "overall_uniqueness = (id_unique+id_article_uniqueness+combo_uniquness)/3\n",
        "print(f\"Overall Uniqueness: {overall_uniqueness:.2f}%\")"
      ],
      "metadata": {
        "id": "xJJK9MB_U5RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validition\n",
        "\n",
        "If data is correct in correct format. Logic defined in column are correct"
      ],
      "metadata": {
        "id": "JsjjqNSpYVZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking data types of the columns\n",
        "final_df.dtypes"
      ],
      "metadata": {
        "id": "0symQ21CYI0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df[[\"amount_normalized\",\"headcount\",\"product_fuzzy_match\"]].dtypes"
      ],
      "metadata": {
        "id": "sVtnS3cOD5rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value variation presnt in Dataframe this will give us more consince view which column need to be corrected"
      ],
      "metadata": {
        "id": "nsYLlCbQIczi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.describe()"
      ],
      "metadata": {
        "id": "1xdRviPdC-EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.amount_normalized.value_counts()"
      ],
      "metadata": {
        "id": "C0v7v6ttCMw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we checked above there were few rows whose found date was before the effective date or published date.\n",
        "This is also a major error in data"
      ],
      "metadata": {
        "id": "YScIlkl8IuXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_validation = final_df.copy()\n",
        "\n",
        "date_validation['found_date'] = pd.to_datetime(date_validation['found_date'])\n",
        "date_validation['effective_date']= pd.to_datetime(date_validation['effective_date'],utc=True)\n",
        "\n",
        "date_validation[\"reference_date\"] = date_validation[\"effective_date\"].fillna(date_validation[\"published_at\"])\n",
        "\n",
        "validation = (date_validation[\"planning\"] == False) & (date_validation[\"effective_date\"].notna()& date_validation[\"reference_date\"].notna())\n",
        "\n",
        "validation_final  = (date_validation.loc[validation,\"reference_date\"] <= date_validation.loc[validation,\"found_date\"]).mean()*100\n",
        "print(f\"date validation :{validation_final:.2f}%\")"
      ],
      "metadata": {
        "id": "tkWijVpYCRxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample of rows where found_date>effectivce date or publsihed date"
      ],
      "metadata": {
        "id": "ENZkYbhTamFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fails = date_validation.loc[\n",
        "    validation & (date_validation[\"found_date\"] < date_validation[\"reference_date\"]),\n",
        "    [\"id\",\"summary\",\"found_date\",\"effective_date\",\"published_at\",\"reference_date\",\"planning\",\"category\",\"summary\",\"confidence_score\",\"human_approved\"]\n",
        "]\n",
        "print(f\"Size of df is {fails.shape}\")\n",
        "fails.head(10)"
      ],
      "metadata": {
        "id": "PDvbefpVKEyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fails.category.value_counts()"
      ],
      "metadata": {
        "id": "-Y-QeVN4gX5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Quality\n",
        "\n",
        "This section basically focus on correcting the data flaws present in data and fixing the issue."
      ],
      "metadata": {
        "id": "gNQEzbnIhVDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Duplicates"
      ],
      "metadata": {
        "id": "0A35DBGthbqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of data frame before removing duplicates {final_df.shape}\")\n",
        "final_df = final_df.drop_duplicates()\n",
        "print(f\"Shape of data frame after removing duplicates {final_df.shape}\")"
      ],
      "metadata": {
        "id": "C12pEqS2gspg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have checked in Duplicacy part we have find article id is also getting duplicated here we will be figuring out the article id with issue and making correction."
      ],
      "metadata": {
        "id": "vq_xsGi2a3Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.id_article.value_counts(ascending=False)"
      ],
      "metadata": {
        "id": "rXMkXDs-h4Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " duplicate_row_check = final_df[final_df['id_article']=='13d038e6-ecaf-4a2f-8ad7-2fd093f8d090']\n",
        " duplicate_row_check.head(6)"
      ],
      "metadata": {
        "id": "CzEYTIXbiFFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove the Duplicate article id we have used row number approach here where we will be sorting our data based on effective date desc and id. To pick and filter the data"
      ],
      "metadata": {
        "id": "uD586WktbQtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.sort_values(by = [\"effective_date\",\"id\"],ascending = [False,True])\n",
        "\n",
        "final_df[\"row_number\"] = final_df.groupby([\"id_article\"]).cumcount()+1\n",
        "print(f\"shape of dataframe{final_df.shape}\")\n",
        "final_df.head(5)\n"
      ],
      "metadata": {
        "id": "B9VFCFZaiITi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only selected the rows which have row number 1 to remove the duplicay in our data"
      ],
      "metadata": {
        "id": "eypbNIRobnPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_filter = final_df[final_df[\"row_number\"]==1]\n",
        "final_df_filter = final_df_filter.drop(\"row_number\",axis=1)\n",
        "final_df_filter.shape"
      ],
      "metadata": {
        "id": "bL2ir96eiQ8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Checking with related columns to see the prescene of duplicacy"
      ],
      "metadata": {
        "id": "SGw6ofLItwEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = final_df_filter[[\"summary\"]].value_counts().reset_index(name=\"occurrence\")\n",
        "counts.head(10)"
      ],
      "metadata": {
        "id": "Z50hnSrVtk76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_filter = final_df_filter.sort_values(by = [\"effective_date\",\"id\"],ascending = [False,True])\n",
        "\n",
        "final_df_filter[\"row_number\"] = final_df_filter.groupby([\"summary\"]).cumcount()+1\n",
        "print(f\"shape of dataframe{final_df_filter.shape}\")"
      ],
      "metadata": {
        "id": "RZhaNSjXt8Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the Rows where summary  is similar"
      ],
      "metadata": {
        "id": "ioWVk-XYvWdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_ = final_df_filter[final_df_filter[\"row_number\"]==1]\n",
        "final_df_ = final_df_.drop(\"row_number\",axis=1)\n",
        "final_df_.shape"
      ],
      "metadata": {
        "id": "k69MTHjruSI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = final_df_[[\"summary\"]].value_counts().reset_index(name=\"occurrence\")\n",
        "counts.head(10)"
      ],
      "metadata": {
        "id": "th_yy2MUuqkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_.id_article.value_counts(ascending=False)"
      ],
      "metadata": {
        "id": "yw-okZFcifaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_.dtypes"
      ],
      "metadata": {
        "id": "L8QNbV9ZjKDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As above dtypes says effective date dtype is db date where will be facing issue whicle having comparison with other date type columns"
      ],
      "metadata": {
        "id": "xKhp2g8Zbz2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_['found_date'] = pd.to_datetime(final_df_['found_date'])\n",
        "final_df_['effective_date']= pd.to_datetime(final_df_['effective_date'],utc=True)"
      ],
      "metadata": {
        "id": "tjC8u0v4ioZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_[[\"effective_date\",\"published_at\",\"found_date\"]].dtypes"
      ],
      "metadata": {
        "id": "KySny_IgjGFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product fuzzy match column is in string dtype but while anlaysing we found out that it is boolean.Fixing the daata type.\n",
        "Also Headcount and amount normalized is in float and negative which can't be possible so correcting the datatype and fixing the negativ error"
      ],
      "metadata": {
        "id": "PEXCie5ncGlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_[\"product_fuzzy_match\"] = final_df_[\"product_fuzzy_match\"].astype(bool)\n",
        "final_df_[[\"amount_normalized\",\"headcount\"]] = final_df_[[\"amount_normalized\",\"headcount\"]].astype(\"Int64\")"
      ],
      "metadata": {
        "id": "Xdp5hK_RjGP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_[\"amount_normalized\"] = pd.to_numeric(final_df_[\"amount_normalized\"],errors = 'coerce')\n",
        "final_df_[\"headcount\"] = pd.to_numeric(final_df_[\"headcount\"],errors = 'coerce')\n",
        "\n",
        "\n",
        "final_df_[\"amount_normalized\"] = final_df_[\"amount_normalized\"].abs()\n",
        "final_df_[\"headcount\"] = final_df_[\"headcount\"].abs()"
      ],
      "metadata": {
        "id": "VtP7O12DjGij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bd2i4Lh4cml2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check of data types\n",
        "final_df_.dtypes"
      ],
      "metadata": {
        "id": "Mi7FgnHelMVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of numerical value\n",
        "final_df_.describe()"
      ],
      "metadata": {
        "id": "4-v_GX7qdjbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_conf = date_accuracy_check[\"confidence_score\"]>0.8\n",
        "\n",
        "# if high confidence is true and human approved  is true or high confidence is false or human approved is false that will give us true\n",
        "agreement = ((high_conf & date_accuracy_check[\"human_approved\"]) | (~high_conf & ~date_accuracy_check[\"human_approved\"]))\n",
        "aggreement_accuracy = agreement.mean()*100\n",
        "\n",
        "print(f\"Confidence and Human approved: {aggreement_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "zX80mdPyfntQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_unique = final_df_[\"id\"].nunique()/len(final_df_)*100\n",
        "print(f\"Uniquesness of id: {id_unique:.2f}%\")\n",
        "\n",
        "id_article_uniqueness = final_df_[\"id_article\"].nunique()/len(final_df_)*100\n",
        "print(f\"article_id uniqueness: {id_article_uniqueness:.2f}%\")\n",
        "\n",
        "\n",
        "combo_uniquness = final_df_[[\"summary\",\"found_date\"]].drop_duplicates().shape[0]/len(final_df_)*100\n",
        "print(f\"combo_uniquness : {combo_uniquness:.2f}%\")\n",
        "\n",
        "overall_uniqueness = (id_unique+id_article_uniqueness+combo_uniquness)/3\n",
        "print(f\"Overall Uniqueness: {overall_uniqueness:.2f}%\")"
      ],
      "metadata": {
        "id": "UXi3t30liThJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_.shape"
      ],
      "metadata": {
        "id": "02YucgrOsr0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8wGcu91ANbVe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}